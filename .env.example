# ENVIRONMENT PARAMETERS
ENV=prod
# dev, test, prod
DATA_STORAGE_ENV=S3
# filesystem, S3
MODEL_STORAGE_ENV=S3
# filesystem, S3, mlflow
ETL_ENV=lambda
# local, docker-compose, lambda
MODEL_BUILDING_ENV=sagemaker
# local, docker-compose, sagemaker
APP_ENV=local
# local, docker-compose, EC2


# AWS PARAMETERS
REGION_NAME=sa-east-1
# us-east-1, sa-east-1
BUCKET_NAME=bucket-name
SECRET_ARN=arn

ETL_LAMBDA_FUNCTION_NAME=etl-prod

LAMBDA_EXECUTION_ROLE_NAME=LambdaExecutionRole-MLAccelerator-Prod
LAMBDA_EXECUTION_ROLE_ARN=arn

SAGEMAKER_EXECUTION_ROLE_NAME=SageMakerExecutionRole-MLAccelerator-Prod
SAGEMAKER_EXECUTION_ROLE_ARN=arn

MODEL_BUILDING_STEP_FUNCTIONS_NAME=ModelBuildingWorkflow_Prod
MODEL_BUILDING_STEP_FUNCTIONS_FILE_NAME=model_building_workflow_prod.json
MODEL_BUILDING_STEP_FUNCTIONS_ARN=arn

STEP_FUNCTIONS_EXECUTION_ROLE_NAME=StepFunctionsExecutionRole-MLAccelerator-Prod
STEP_FUNCTIONS_EXECUTION_ROLE_ARN=arn


# API KEYS
KXY_API_KEY=key


# DOCKER REPOSITORY PARAMETERS
DOCKER_REPOSITORY_TYPE=ECR
# dockerhub, ECR
DOCKER_REPOSITORY_NAME=repository-name
# ml-accelerator-ecr, ml-accelerator-dockerhub
ECR_REPOSITORY_URI=uri
DOCKERHUB_USERNAME=username
DOCKERHUB_TOKEN=token


# INFRASTRUCTURE PARAMETERS
ETL_LAMBDA_FUNCTION_MEMORY_SIZE=512
ETL_LAMBDA_FUNCTION_TIMEOUT=300

PROCESSING_INSTANCE_TYPE=ml.t3.large
PROCESSING_INSTANCE_COUNT=1
PROCESSING_VOLUME_SIZE=30
PROCESSING_MAX_RUNTIME=300

TUNING_INSTANCE_TYPE=ml.t3.large
TUNING_INSTANCE_COUNT=1
TUNING_VOLUME_SIZE=30
TUNING_MAX_RUNTIME=300

TRAINING_INSTANCE_TYPE=ml.t3.large
TRAINING_INSTANCE_COUNT=1
TRAINING_VOLUME_SIZE=30
TRAINING_MAX_RUNTIME=300

EVALUATING_INSTANCE_TYPE=ml.t3.large
EVALUATING_INSTANCE_COUNT=1
EVALUATING_VOLUME_SIZE=30
EVALUATING_MAX_RUNTIME=300

INFERENCE_INSTANCE_TYPE=ml.t3.large
INFERENCE_INSTANCE_COUNT=1
INFERENCE_VOLUME_SIZE=30
INFERENCE_MAX_RUNTIME=300

DEFAULT_INSTANCE_TYPE=ml.t3.large
DEFAULT_INSTANCE_COUNT=1
DEFAULT_VOLUME_SIZE=30
DEFAULT_MAX_RUNTIME=300

# Instance types: https://aws.amazon.com/sagemaker/pricing/instance-types/
# - ml.t3.large:    2 vCPU   8 GiB


# DEPLOYMENT PARAMETERS
INFERENCE_HOST=0.0.0.0
INFERENCE_PORT=8080
WEBAPP_HOST=localhost
# 0.0.0.0, localhost
WEBAPP_PORT=8501


# PATHS
RAW_DATASETS_PATH=datasets/raw
PROCESSING_DATASETS_PATH=datasets/processing
INFERENCE_PATH=inference
TRANSFORMERS_PATH=transformers
MODELS_PATH=models
SCHEMAS_PATH=schemas
STEP_FUNCTIONS_PATH=step_functions
MOCK_PATH=mock


# OTHERS
SEED=23111997