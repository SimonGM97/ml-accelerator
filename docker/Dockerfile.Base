# Use a python 3.10-slim base image
FROM python:3.10.8-slim

# Set the working directory to /app
WORKDIR /tmp

# Upgrade pip
RUN pip install --upgrade pip

# Install git (required by GitPython)
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

# Install libgomp (GNU OpenMP library) required by LightGBM
RUN apt-get update && apt-get install -y libgomp1 && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Updates the package lists for the APT (Advanced Package Tool) package management system.
RUN apt-get update

# Install requirements
RUN pip install -r requirements.txt

# Install dask[dataframe]
RUN pip install "dask[dataframe]"

# Copy specified files to install ml_accelerator
COPY ml_accelerator ml_accelerator
COPY README.md .
COPY LICENSE .
COPY setup.py .

# Install ml_accelerator library
RUN pip install --no-cache-dir .

# Switch back to /app dir
WORKDIR /app

# Define build arguments
ARG ENV
ARG DATA_STORAGE_ENV
ARG MODEL_STORAGE_ENV
ARG ETL_ENV
ARG MODEL_BUILDING_ENV
ARG APP_ENV

ARG REGION_NAME
ARG BUCKET_NAME

ARG KXY_API_KEY

ARG INFERENCE_HOST
ARG INFERENCE_PORT
ARG WEBAPP_HOST
ARG WEBAPP_PORT

ARG RAW_DATASETS_PATH
ARG PROCESSING_DATASETS_PATH
ARG INFERENCE_PATH
ARG TRANSFORMERS_PATH
ARG MODELS_PATH
ARG SCHEMAS_PATH
ARG MOCK_PATH

ARG SEED

# Set environment variables from the build argument
ENV ENV=${ENV}
ENV DATA_STORAGE_ENV=${DATA_STORAGE_ENV}
ENV MODEL_STORAGE_ENV=${MODEL_STORAGE_ENV}

ENV REGION_NAME=${REGION_NAME}
ENV BUCKET_NAME=${BUCKET_NAME}

ENV KXY_API_KEY=${KXY_API_KEY}

ENV INFERENCE_HOST=${INFERENCE_HOST}
ENV INFERENCE_PORT=${INFERENCE_PORT}
ENV WEBAPP_HOST=${WEBAPP_HOST}
ENV WEBAPP_PORT=${WEBAPP_PORT}

ENV RAW_DATASETS_PATH=${RAW_DATASETS_PATH}
ENV PROCESSING_DATASETS_PATH=${PROCESSING_DATASETS_PATH}
ENV INFERENCE_PATH=${INFERENCE_PATH}
ENV TRANSFORMERS_PATH=${TRANSFORMERS_PATH}
ENV MODELS_PATH=${MODELS_PATH}
ENV SCHEMAS_PATH=${SCHEMAS_PATH}
ENV MOCK_PATH=${MOCK_PATH}

ENV SEED=${SEED}

# Copy required files & directories
COPY config config
COPY .env .env
COPY .aws /root/.aws
COPY resources resources

# TRAINING JOBS directory convention:
# /opt/ml/input/: This is typically used for training jobs to store input data.
# /opt/ml/model/: For training jobs, this directory is where trained models are stored.
# /opt/ml/output/: This is used for outputs from processing or training jobs.

# Create training job directories inside the container
RUN mkdir -p /opt/ml/input && \
    mkdir -p /opt/ml/output && \
    mkdir -p /opt/ml/model

# PROCESSING JOBS directory convention:
# /opt/ml/processing/input/: This is where input data from S3 is downloaded.
# /opt/ml/processing/output/: This is where output data should be written (if your processing job produces results).
# Other optional outputs:
#   - /opt/ml/processing/train/: train datasets
#   - /opt/ml/processing/validation/: validation datasets
#   - /opt/ml/processing/test/: test datasets
#   - /opt/ml/processing/model/: model artifacts
#   - /opt/ml/processing/evaluation/: evaluation data

# Create processing job directories inside the container
RUN mkdir -p /opt/ml/processing/input && \
    mkdir -p /opt/ml/processing/output && \
    mkdir -p /opt/ml/processing/train && \
    mkdir -p /opt/ml/processing/validation && \
    mkdir -p /opt/ml/processing/test && \
    mkdir -p /opt/ml/processing/model && \
    mkdir -p /opt/ml/processing/evaluation

# Define volumes
VOLUME ["/app/${BUCKET_NAME}", "/app/config"]
VOLUME ["/opt/ml/input", "/opt/ml/output", "/opt/ml/model"]
VOLUME ["/opt/ml/processing/input", "/opt/ml/processing/output"]
VOLUME ["/opt/ml/processing/train", "/opt/ml/processing/validation", "/opt/ml/processing/test"]
VOLUME ["/opt/ml/processing/model", "/opt/ml/processing/evaluation"]

# Expose inference & webapp ports
EXPOSE ${INFERENCE_PORT}
EXPOSE ${WEBAPP_PORT}